{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ayyub Alzahem 3700203"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Porter Stemmer in python nltk package \n",
    " \n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exercise 1: Demonstrate how can you apply the Porter\n",
    "Stemmer for finding the stem of word “Understanding” and\n",
    "“Demonstration”! </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise 2: Assume you have a list of words that you want\n",
    "to get their stem, write python script that find stem of\n",
    "each word in the list. \n",
    " </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "program\n",
      "program\n",
      "program\n",
      "cake\n",
      "indic\n",
      "matric\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "Stemmerporter = PorterStemmer() \n",
    "\n",
    "list=['dogs', 'programming', 'programs', 'programmed',\n",
    "      'cakes', 'indices', 'matrices']\n",
    "for word in list:\n",
    "    print(Stemmerporter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs            -> dog\n",
      "programming     -> program\n",
      "programs        -> program\n",
      "programmed      -> program\n",
      "cakes           -> cake\n",
      "indices         -> indic\n",
      "matrices        -> matric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayyub\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## For more Efficient Output\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "porter = PorterStemmer()\n",
    "l_words = ['dogs', 'programming', 'programs', 'programmed', 'cakes', 'indices', 'matrices']\n",
    "\n",
    "## print each word in the sentence before and after Stemming \n",
    "for word in l_words:\n",
    "    print(f'{word} \\t -> {porter.stem(word)}'.expandtabs(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exercise 3: Now, apply Poster Stemmer on the following\n",
    "sentence: \n",
    "</h3>\n",
    "Hint: do tokenization first to find list of all words in\n",
    "the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A               -> A\n",
      "stemmer         -> stemmer\n",
      "for             -> for\n",
      "English         -> english\n",
      "operating       -> oper\n",
      "on              -> on\n",
      "the             -> the\n",
      "stem            -> stem\n",
      "cat             -> cat\n",
      "sh              -> sh\n",
      "ould            -> ould\n",
      "identify        -> identifi\n",
      "such            -> such\n",
      "strings         -> string\n",
      "as              -> as\n",
      "cats            -> cat\n",
      ",               -> ,\n",
      "catlike         -> catlik\n",
      ",               -> ,\n",
      "and             -> and\n",
      "catty.          -> catty.\n",
      "A               -> A\n",
      "stem            -> stem\n",
      "ming            -> ming\n",
      "algorithm       -> algorithm\n",
      "might           -> might\n",
      "also            -> also\n",
      "reduce          -> reduc\n",
      "the             -> the\n",
      "words           -> word\n",
      "fishing         -> fish\n",
      ",               -> ,\n",
      "fished          -> fish\n",
      ",               -> ,\n",
      "an              -> an\n",
      "d               -> d\n",
      "fisher          -> fisher\n",
      "to              -> to\n",
      "the             -> the\n",
      "stem            -> stem\n",
      "fish.           -> fish.\n",
      "The             -> the\n",
      "stem            -> stem\n",
      "need            -> need\n",
      "not             -> not\n",
      "be              -> be\n",
      "a               -> a\n",
      "word            -> word\n",
      ",               -> ,\n",
      "for             -> for\n",
      "ex              -> ex\n",
      "ample           -> ampl\n",
      "the             -> the\n",
      "Porter          -> porter\n",
      "algorithm       -> algorithm\n",
      "reduces         -> reduc\n",
      ",               -> ,\n",
      "argue           -> argu\n",
      ",               -> ,\n",
      "argued          -> argu\n",
      ",               -> ,\n",
      "argues          -> argu\n",
      ",               -> ,\n",
      "arg             -> arg\n",
      "uing            -> u\n",
      ",               -> ,\n",
      "and             -> and\n",
      "argus           -> argu\n",
      "to              -> to\n",
      "the             -> the\n",
      "stem            -> stem\n",
      "argu            -> argu\n",
      ".               -> .\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "sentence = '''A stemmer for English operating on the stem cat sh\n",
    "ould identify such strings as cats, catlike, and catty. A stem\n",
    "ming algorithm might also reduce the words fishing, fished, an\n",
    "d fisher to the stem fish. The stem need not be a word, for ex\n",
    "ample the Porter algorithm reduces, argue, argued, argues, arg\n",
    "uing, and argus to the stem argu.'''\n",
    "\n",
    "## tokenize the sentence\n",
    "list = tokenizer.tokenize(sentence)\n",
    "\n",
    "## print each word in the sentence before and after Stemming \n",
    "for word in list:\n",
    "    print(f'{word} \\t -> {porter.stem(word)}'.expandtabs(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A stemmer for english oper on the stem cat sh ould identifi such string as cat , catlik , and catti . A stem ming algorithm might also reduc the word fish , fish , an d fisher to the stem fish . the stem need not be a word , for ex ampl the porter algorithm reduc , argu , argu , argu , arg u , and argu to the stem argu .'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Another Format for the Output\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentence = '''A stemmer for English operating on the stem cat sh\n",
    "ould identify such strings as cats, catlike, and catty. A stem\n",
    "ming algorithm might also reduce the words fishing, fished, an\n",
    "d fisher to the stem fish. The stem need not be a word, for ex\n",
    "ample the Porter algorithm reduces, argue, argued, argues, arg\n",
    "uing, and argus to the stem argu.'''\n",
    "\n",
    "## tokenize the sentence\n",
    "tokenized_words = word_tokenize(sentence)\n",
    "tokenized_sentence = []\n",
    "for word in tokenized_words:\n",
    "    tokenized_sentence.append(porter.stem(word))\n",
    "    \n",
    "tokenized_sentence = \" \".join(tokenized_sentence)\n",
    "\n",
    "## print each word in the sentence before and after Stemming \n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Lancaster Stemmer in python nltk package \n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exercise 3: Repeat the task presented in exercise 2. Then,\n",
    "compare outputs! Do you find a difference between Porter\n",
    "Stemmer outputs and Lancaster Stemmer? Why??? </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "program\n",
      "program\n",
      "program\n",
      "cak\n",
      "ind\n",
      "mat\n",
      "===============================\n",
      "dog\n",
      "program\n",
      "program\n",
      "program\n",
      "cake\n",
      "indic\n",
      "matric\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "StemmerLancaster = LancasterStemmer() \n",
    "Stemmerporter = PorterStemmer() \n",
    "\n",
    "list=['dogs', 'programming', 'programs', 'programmed',\n",
    "      'cakes', 'indices', 'matrices']\n",
    "\n",
    "for word in list:\n",
    "    print(StemmerLancaster.stem(word))\n",
    "    \n",
    "print('===============================')\n",
    "\n",
    "for word in list:\n",
    "    print(Stemmerporter.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Ecplaination: </b>because the lancaster stemmer is significantly more aggressive than the porter stemmer.\n",
    "<br><br>\n",
    "<b>Porter:</b> Most commonly used stemmer without a doubt, also one of the most gentle stemmers. One of the few stemmers that actually has Java support which is a plus, though it is also the most computationally intensive of the algorithms(Granted not by a very significant margin). It is also the oldest stemming algorithm by a large margin.<br><br>\n",
    "<b>Lancaster:</b> Very aggressive stemming algorithm, sometimes to a fault. With porter and snowball, the stemmed representations are usually fairly intuitive to a reader, not so with Lancaster, as many shorter words will become totally obfuscated. The fastest algorithm here, and will reduce your working set of words hugely, but if you want more distinction, not the tool you would want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exercise 4: Check how to perform stemming using\n",
    "SnowballStemmer and Write your python script below! </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "program\n",
      "program\n",
      "program\n",
      "cake\n",
      "indic\n",
      "matric\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "StemmerSnowball = SnowballStemmer('english') \n",
    "\n",
    "list=['dogs', 'programming', 'programs', 'programmed',\n",
    "      'cakes', 'indices', 'matrices']\n",
    "\n",
    "for word in list:\n",
    "    print(StemmerSnowball.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Stemming in other Languages </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حرك\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "st = ISRIStemmer()\n",
    "w = 'حركات'\n",
    "\n",
    "print(st.stem(w)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Stemming a document \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A stemmer for English operating on the stem cat sh\n",
      "ould identify such strings as cats, catlike, and catty. A stem\n",
      "ming algorithm might also reduce the words fishing, fished, an\n",
      "d fisher to the stem fish. The stem need not be a word, for ex\n",
      "ample the Porter algorithm reduces, argue, argued, argues, arg\n",
      "uing, and argus to the stem argu.\n",
      "Stemmed sentence\n",
      "A stemmer for english oper on the stem cat sh ould identifi such string as cat , catlik , and catti . A stem ming algorithm might also reduc the word fish , fish , an d fisher to the stem fish . the stem need not be a word , for ex ampl the porter algorithm reduc , argu , argu , argu , arg u , and argu to the stem argu . \n"
     ]
    }
   ],
   "source": [
    "file=open(r'C:\\Users\\ayyub\\Desktop\\file.txt',encoding = 'UTF-8')\n",
    "Sentences= file.read()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words: \n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "    \n",
    "print(Sentences)\n",
    "print(\"Stemmed sentence\")\n",
    "x = stemSentence(Sentences)\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lemmatization \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ayyub\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = '''He was running and eating at same time. He has bad\n",
    "habit of swimming after playing long hours in the Sun.'''\n",
    "\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "    \n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exercise 5. Now compare the output? What did you see? Why? </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))\n",
    "\n",
    "## v --> verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Explaination:</b> because we have to use Parts of Speech.<br><br>\n",
    "The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging. Parts of speech are also known as word classes or lexical categories. The collection of tags used for a particular task is known as a tagset. Our emphasis in this chapter is on exploiting tags, and tagging text automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exercise 6. Demonstrate how to use ISRIStemmer for stemming\n",
    "Arabic document. \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حركات بركات\n",
      "Stemmed sentence\n",
      "حرك برك \n"
     ]
    }
   ],
   "source": [
    "## Arabic\n",
    "file=open(r'C:\\Users\\ayyub\\Desktop\\file.txt',encoding = 'UTF-8')\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "st = ISRIStemmer()\n",
    "\n",
    "Sentences= file.read()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words: \n",
    "        stem_sentence.append(st.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "    \n",
    "print(Sentences)\n",
    "print(\"Stemmed sentence\")\n",
    "x = stemSentence(Sentences)\n",
    "print(x) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
