{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ayyub Alzahem\n",
    "3700203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.4\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.18.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.version.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.0\n"
     ]
    }
   ],
   "source": [
    "import gensim as gs\n",
    "print(gs.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exercise 2: Tokenize the following sentence and write down the result you obtain! </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function tokenize in module gensim.utils:\n",
      "\n",
      "tokenize(text, lowercase=False, deacc=False, encoding='utf8', errors='strict', to_lower=False, lower=False)\n",
      "    Iteratively yield tokens as unicode strings, optionally removing accent marks and lowercasing it.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    text : str or bytes\n",
      "        Input string.\n",
      "    deacc : bool, optional\n",
      "        Remove accentuation using :func:`~gensim.utils.deaccent`?\n",
      "    encoding : str, optional\n",
      "        Encoding of input string, used as parameter for :func:`~gensim.utils.to_unicode`.\n",
      "    errors : str, optional\n",
      "        Error handling behaviour, used as parameter for :func:`~gensim.utils.to_unicode`.\n",
      "    lowercase : bool, optional\n",
      "        Lowercase the input string?\n",
      "    to_lower : bool, optional\n",
      "        Same as `lowercase`. Convenience alias.\n",
      "    lower : bool, optional\n",
      "        Same as `lowercase`. Convenience alias.\n",
      "    \n",
      "    Yields\n",
      "    ------\n",
      "    str\n",
      "        Contiguous sequences of alphabetic characters (no digits!), using :func:`~gensim.utils.simple_tokenize`\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.utils import tokenize\n",
      "        >>> list(tokenize('Nic nemůže letět rychlostí vyšší, než 300 tisíc kilometrů za sekundu!', deacc=True))\n",
      "        [u'Nic', u'nemuze', u'letet', u'rychlosti', u'vyssi', u'nez', u'tisic', u'kilometru', u'za', u'sekundu']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gs.utils.tokenize\n",
    "\n",
    "help(gs.utils.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'breaking',\n",
       " 'down',\n",
       " 'text',\n",
       " 'documentapart',\n",
       " 'into',\n",
       " 'those',\n",
       " 'pieces']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentence= '''Tokenization is the process of breaking\n",
    "down text documentapart into those pieces'''\n",
    "\n",
    "import gensim as gs\n",
    "tokenizedWord= list(gs.utils.tokenize(Sentence)) \n",
    "tokenizedWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exercise 3: Count frequency of each word. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 3), (26, 1), (27, 1), (28, 1), (29, 3), (30, 1), (31, 1), (32, 1), (33, 1), (34, 2), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 2), (44, 2), (45, 1)]]\n"
     ]
    }
   ],
   "source": [
    "document  = ''' In computer science, artificial\n",
    "intelligence (AI), sometimes called machine\n",
    "intelligence, is intelligence demonstrated by\n",
    "machines, in contrast to the natural intelligence\n",
    "displayed by humans and animals. Computer science\n",
    "defines AI research as the study of intelligent\n",
    "agents: any device that perceives its environment and\n",
    "takes actions that maximize its chance of successfully\n",
    "achieving its goals.” \n",
    "'''\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "text = [document]\n",
    "tokens = [[token for token in sentence.split()] for sentence\n",
    "in text]\n",
    "gensim_dictionary = corpora.Dictionary()\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(token,\n",
    "allow_update=True) for token in tokens]\n",
    "print(gensim_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('(AI),', 1), ('AI', 1), ('Computer', 1), ('In', 1), ('achieving', 1), ('actions', 1), ('agents:', 1), ('and', 2), ('animals.', 1), ('any', 1), ('artificial', 1), ('as', 1), ('by', 2), ('called', 1), ('chance', 1), ('computer', 1), ('contrast', 1), ('defines', 1), ('demonstrated', 1), ('device', 1), ('displayed', 1), ('environment', 1), ('goals.”', 1), ('humans', 1), ('in', 1), ('intelligence', 3), ('intelligence,', 1), ('intelligent', 1), ('is', 1), ('its', 3), ('machine', 1), ('machines,', 1), ('maximize', 1), ('natural', 1), ('of', 2), ('perceives', 1), ('research', 1), ('science', 1), ('science,', 1), ('sometimes', 1), ('study', 1), ('successfully', 1), ('takes', 1), ('that', 2), ('the', 2), ('to', 1)]]\n"
     ]
    }
   ],
   "source": [
    "word_frequencies = [[(gensim_dictionary[id], frequence) \n",
    "                     for id, frequence in couple] \n",
    "                    for couple in gensim_corpus]\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see? <br>\n",
    "we notice that there are some of the words are repeted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "\n",
    "tokens = [simple_preprocess(sentence, deacc=True) \n",
    "          for sentence in open(r'C:\\Users\\ayyub\\Desktop\\file.txt',\n",
    "                               encoding='utf-8')]\n",
    "\n",
    "gensim_dictionary = corpora.Dictionary()\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(token, allow_update=True)\n",
    "                 for token in tokens]\n",
    "word_frequencies = [[(gensim_dictionary[id], frequence) \n",
    "                     for id, frequence in couple] \n",
    "                    for couple in gensim_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Home exercise: </H3>\n",
    "Create a bag of words corpus by reading a text file.\n",
    "Hint: use genism.utils.simple_preprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1)]\n",
      "[(4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n",
      "[(6, 2), (9, 1), (10, 1), (11, 1)]\n",
      "[(2, 1), (6, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)]\n",
      "[(1, 1), (3, 1), (9, 1), (17, 1), (18, 1), (19, 1), (20, 1)]\n",
      "[(4, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1)]\n",
      "[(17, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1)]\n",
      "[(24, 1), (31, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1)]\n",
      "[(31, 1), (39, 1), (40, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ayyub\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\ayyub\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import nltk\n",
    "nltk.download('stopwords')  # run once\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "class BoWCorpus(object):\n",
    "    def __init__(self, path, dictionary):\n",
    "        self.filepath = path\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    def __iter__(self):\n",
    "        global mydict  # OPTIONAL, only if updating the source dictionary.\n",
    "        for line in smart_open(self.filepath, encoding='latin'):\n",
    "            # tokenize\n",
    "            tokenized_list = simple_preprocess(line, deacc=True)\n",
    "\n",
    "            # create bag of words\n",
    "            bow = self.dictionary.doc2bow(tokenized_list, allow_update=True)\n",
    "\n",
    "            # update the source dictionary (OPTIONAL)\n",
    "            mydict.merge_with(self.dictionary)\n",
    "\n",
    "            # lazy return the BoW\n",
    "            yield bow\n",
    "\n",
    "\n",
    "# Create the Dictionary\n",
    "mydict = corpora.Dictionary()\n",
    "\n",
    "# Create the Corpus\n",
    "bow_corpus = BoWCorpus(r'C:\\Users\\ayyub\\Desktop\\file.txt',\n",
    "                       dictionary=mydict)  # memory friendly\n",
    "\n",
    "# Print the token_id and count for each line.\n",
    "for line in bow_corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
